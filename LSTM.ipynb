{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radonys/Deep-Learning-Assignments/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5Ahij3CyOOgG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install pandas nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xHC_xr74Ogae",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpo64aO7QaYA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def string_form(value):\n",
        "    return str(value).lower()\n",
        "\n",
        "def clean_text(text):\n",
        "   \n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text\n",
        "\n",
        "def pad_features(reviews_int, seq_length=200):\n",
        "    \n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
        "    \n",
        "    for i, review in enumerate(reviews_int):\n",
        "        \n",
        "        review_len = len(review)\n",
        "        \n",
        "        if review_len <= seq_length:\n",
        "            \n",
        "            zeroes = list(np.zeros(seq_length-review_len))\n",
        "            new = zeroes+review\n",
        "        \n",
        "        elif review_len > seq_length:\n",
        "            \n",
        "            new = review[0:seq_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxzyYwtUPL_T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_table('mrdata.tsv')\n",
        "data['Phrase'] = data['Phrase'].apply(string_form)\n",
        "data['Phrase'] = data['Phrase'].apply(clean_text)\n",
        "del data['PhraseId']\n",
        "del data['SentenceId']\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtP6QcqoTo1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_text = ' '.join(data['Phrase'])\n",
        "words = all_text.split()\n",
        "count_words = Counter(words)\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)\n",
        "print(count_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzYHCX6aUPO_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "reviews_int = []\n",
        "for review in data['Phrase']:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_int.append(r)\n",
        "print (reviews_int[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbo8_vPTVCUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_labels = [int(i) for i in data['Sentiment']]\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pN3TgOniWFIu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews_len = [len(x) for x in reviews_int]\n",
        "pd.Series(reviews_len).hist()\n",
        "plt.show()\n",
        "pd.Series(reviews_len).describe()\n",
        "\n",
        "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0]\n",
        "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0]\n",
        "features = pad_features(reviews_int)\n",
        "print(features[:10,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CoUJC9l0aJ4O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "train_x = features[0:int(split_frac*len_feat)]\n",
        "train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
        "test_x = features[int(split_frac*len_feat):]\n",
        "test_y = encoded_labels[int(split_frac*len_feat):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6pTtcL9racST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}