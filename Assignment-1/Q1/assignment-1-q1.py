# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w3b9WmP7DizIw-Nb3MhI2d1RcDAFraKp

# Question 1 - Assignment 1

The first task is to design a Multi-Layer Perceptron for XOR gate operation.

The Multi-Layer Perceptron (MLP) for XOR gate operation can be visually analyzed with 2 input neurons, 2 hidden neurons, and 1 output neuron. Along with this, the weights are learned through the Stochastic Gradient Descent. For activations, I have used the Tanh function whose range lies from (-1, 1). The model was trained for 10000 epochs with a learning rate of 0.1.

The output/predictions largely depend upon the initialization of the weights and biases which have led to varied results.

### Imports
"""

import numpy as np

"""### Tanh Activation Function and its Derivative"""

def tanh(x):
    return (1.0 - np.exp(-2*x))/(1.0 + np.exp(-2*x))

def tanh_derivative(x):
    return (1 + tanh(x))*(1 - tanh(x))

"""### Variable Intialization"""

X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])

epoch = 10000
lr = 0.1
inputlayer_neurons = X.shape[1]
hiddenlayer_neurons = 2 
outputlayer_neurons = 1

weight_hidden = np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))
bias_hidden = np.random.uniform(size=(1,hiddenlayer_neurons))
weight_out = np.random.uniform(size=(hiddenlayer_neurons,output_neurons))
bias_out = np.random.uniform(size=(1,output_neurons))

"""### Train the Network"""

for i in range(epoch):

  #Forward-Pass
  
  hidden_layer_input = np.dot(X,weight_hidden)
  hidden_layer_input = hidden_layer_input + bias_hidden
  hiddenlayer_activations = sigmoid(hidden_layer_input)
  output_layer_input = np.dot(hiddenlayer_activations,weight_out)
  output_layer_input= output_layer_input + bias_out
  output = tanh(output_layer_input)

  #Back-Propagation
  
  error = y - output
  slope_output_layer = tanh_derivative(output)
  slope_hidden_layer = tanh_derivative(hiddenlayer_activations)
  d_output = error * slope_output_layer
  error_at_hidden_layer = d_output.dot(weight_out.T)
  d_hiddenlayer = error_at_hidden_layer * slope_hidden_layer
  
  #Stochastic-Gradient-Descent
  
  weight_out += hiddenlayer_activations.T.dot(d_output) * lr
  bias_out += np.sum(d_output, axis=0,keepdims=True) * lr
  weight_hidden += X.T.dot(d_hiddenlayer) * lr
  bias_hidden += np.sum(d_hiddenlayer, axis=0, keepdims=True) * lr

"""### Display Output"""

print(output)
print([i[0] > 0 for i in output])