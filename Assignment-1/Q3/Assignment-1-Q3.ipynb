{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1-Q3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "eNHdOgaouCI9",
        "PMDSpo8MuCJB",
        "gC-rmW5CuCJF",
        "u9ZOf6uduCJL",
        "v3e94S4ruCJR",
        "7d9D40XkuCJc",
        "GBC53iGbuCJh",
        "1Z6nWmlVuCJo",
        "6F5drVe_uCJv",
        "Ro9b2tHpuCJy",
        "hUkrLrPEuCJ6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "B1_j-6kDuCI6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Question 3 - Assignment 1"
      ]
    },
    {
      "metadata": {
        "id": "TOdri3kluCI8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The third task is to provide the sentiment analysis of the text obtained from the Rotten Tomatoes dataset. The sentiments can be classified into 5 categories.\n",
        "\n",
        "The task is achieved using a 3-layer LSTM with a fully connected layer to give the 5 outputs. Cross-Entropy Loss function is used along with the Stochastic Gradient Descent optimizer.\n",
        "\n",
        "The code has been written using the PyTorch Deep Learning Framework."
      ]
    },
    {
      "metadata": {
        "id": "eNHdOgaouCI9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xHC_xr74Ogae",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMDSpo8MuCJB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM Class Definition for Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "ZFrs3_DjuCJC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocabulary_size, output_size, embedding_dimension, hidden_dimension, number_layers, drop_prob=0.5):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.number_layers = number_layers\n",
        "        self.hidden_dimension = hidden_dimension\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension)\n",
        "        self.lstm = nn.LSTM(embedding_dimension, hidden_dimension, number_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dimension, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        embeddings = self.embedding(x)\n",
        "        lstm_output, hidden_state = self.lstm(embeddings, hidden_state)\n",
        "        \n",
        "        lstm_output = lstm_output[:,-1,:]\n",
        "        output = self.dropout(lstm_output)\n",
        "        output = self.fc(output)\n",
        "        sigmoid_output = self.sig(output)\n",
        "        \n",
        "        sigmoid_output = sigmoid_output.view(batch_size, -1)\n",
        "        \n",
        "        return sigmoid_output, hidden_state\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if gpu:\n",
        "            hidden_state = (weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_().cuda(),\n",
        "                  weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_().cuda())\n",
        "        else:\n",
        "            hidden_state = (weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_(),\n",
        "                      weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_())\n",
        "        \n",
        "        return hidden_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gC-rmW5CuCJF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper Functions for Text"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xpo64aO7QaYA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "   \n",
        "    text = str(text).lower()\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text\n",
        "\n",
        "def padding_features(string_int, sequence_length=100):\n",
        "    \n",
        "    features = np.zeros((len(string_int), sequence_length), dtype = int)\n",
        "    \n",
        "    for i, string in enumerate(string_int):\n",
        "        \n",
        "        string_len = len(string)\n",
        "        \n",
        "        if string_len <= sequence_length:\n",
        "            \n",
        "            zeroes = list(np.zeros(sequence_length-string_len))\n",
        "            new = zeroes+string\n",
        "        \n",
        "        elif string_len > sequence_length:\n",
        "            \n",
        "            new = string[0:sequence_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u9ZOf6uduCJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read Data & Processing"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hxzyYwtUPL_T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_table('mrdata.tsv')\n",
        "data['Phrase'] = data['Phrase'].apply(clean_text)\n",
        "del data['PhraseId']\n",
        "del data['SentenceId']\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3e94S4ruCJR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoded Text to Integers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TtP6QcqoTo1w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_text = ' '.join(data['Phrase'])\n",
        "words = all_text.split()\n",
        "count_words = Counter(words)\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tzYHCX6aUPO_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "string_int = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLN5RE3YuCJZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for string in data['Phrase']:\n",
        "    \n",
        "    r = [vocabulary_to_int[w] for w in string.split()]\n",
        "    string_int.append(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7d9D40XkuCJc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode Labels"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vbo8_vPTVCUE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_labels = np.array([int(i) for i in data['Sentiment']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GBC53iGbuCJh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Truncate & Process Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pN3TgOniWFIu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "string_length = [len(x) for x in string_int]\n",
        "string_int = [string_int[i] for i, l in enumerate(string_length) if l>0]\n",
        "encoded_labels = [encoded_labels[i] for i, l in enumerate(string_length) if l>0]\n",
        "features = padding_features(string_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Z6nWmlVuCJo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split Data into Train & Test Datasets"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CoUJC9l0aJ4O",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "len_feat = len(features)\n",
        "train_x = np.array(features[0:int(split_frac*len_feat)])\n",
        "train_y = np.array(encoded_labels[0:int(split_frac*len_feat)])\n",
        "test_x = np.array(features[int(split_frac*len_feat):])\n",
        "test_y = np.array(encoded_labels[int(split_frac*len_feat):])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6pTtcL9racST",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6F5drVe_uCJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Variable Declaration"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Kd3vJ2chF-vy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(vocabulary_to_int)+1\n",
        "output_size = 5\n",
        "embedding_dimension = 400\n",
        "hidden_dimension = 256\n",
        "number_layers = 3\n",
        "gpu = False\n",
        "lr=0.001\n",
        "net = LSTM(vocabulary_size, output_size, embedding_dimension, hidden_dimension, number_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ro9b2tHpuCJy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss Function and Optimizer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gzFv_EIlGZRa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "counter = 0\n",
        "clip = 5\n",
        "\n",
        "if gpu:\n",
        "  net.cuda()\n",
        "\n",
        "net.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pxuyQ9iuCJ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the Network"
      ]
    },
    {
      "metadata": {
        "id": "_mQiKd0vuCJ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for e in range(epochs):\n",
        "    \n",
        "    hidden_state = net.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        \n",
        "        counter += 1\n",
        "\n",
        "        if gpu:\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        hidden_state = tuple([each.data for each in hidden_state])\n",
        "\n",
        "        net.zero_grad()\n",
        "\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        \n",
        "        if((inputs.shape[0],inputs.shape[1]) != (batch_size,100)):\n",
        "          \n",
        "          print('Input Shape Issue:', inputs.shape)\n",
        "          continue\n",
        "        \n",
        "        output, hidden_state = net(inputs, hidden_state)\n",
        "        \n",
        "        loss = criterion(output.squeeze(), labels)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hUkrLrPEuCJ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute Test Results"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jsGZ1O51HVEz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "hidden_state = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    hidden_state = tuple([each.data for each in hidden_state])\n",
        "\n",
        "    if gpu:\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    inputs = inputs.type(torch.LongTensor)\n",
        "    \n",
        "    if((inputs.shape[0],inputs.shape[1]) != (batch_size,100)):\n",
        "          \n",
        "          print('Input Shape Issue:', inputs.shape)\n",
        "          continue\n",
        "    \n",
        "    output, hidden_state = net(inputs, hidden_state)\n",
        "    \n",
        "    test_loss = criterion(output.squeeze(), labels)\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    out = F.softmax(output)\n",
        "    out = out.max(dim=1)[1]\n",
        "    \n",
        "    correct_tensor = out.eq(labels)\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "print(\"Test Loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "test_accuracy = num_correct/len(test_loader.dataset)\n",
        "print(\"Test Accuracy: {:.3f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}